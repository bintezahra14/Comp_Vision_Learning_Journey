{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUqczjHJ9XLY5bdG10XWVx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bintezahra14/Comp_Vision_Learning_Journey/blob/main/Real_world_Application_of_Supervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project i am using the **Student Performance Dataset** from the UCI Machine Learning Repository. It’s small, interesting, and allows us to explore social science topics in education, such as predicting student performance based on socio-economic and school-related features.\n",
        "\n",
        "**Dataset:**\n",
        "Student Performance Dataset\n",
        "\n",
        "This dataset contains data on students' performance in two Portuguese secondary schools, covering features like student demographics, socio-economic status, and prior academic performance.\n",
        "\n",
        "**Project Overview:**\n",
        "We will aim to predict whether a student will pass or fail based on their socio-economic and academic factors using two supervised learning models. We’ll perform the following steps:\n",
        "\n",
        "***1. Problem Statement:***\n",
        "Clearly define the prediction problem.We aim to predict whether a student will pass or fail their final exam based on demographic, social, and academic factors.\n",
        "\n",
        "***2. Data Preprocessing:***\n",
        "Download the dataset and load it using pandas.\n",
        "\n",
        "***Handle Missing Values***\n",
        "Check if the dataset contains any missing values and decide on how to handle them (e.g., imputation or removal).\n",
        "\n",
        "***Encode Categorical Variables***\n",
        "The dataset contains categorical variables like school, sex, address, etc. Use one-hot encoding to convert these variables into numeric format.\n",
        "\n",
        "***Split the Data***\n",
        "Divide the dataset into features (X) and target (y), where y will be a binary indicator for pass/fail.Use train_test_split() to divide the data into training and testing sets.\n",
        "\n",
        "***Feature Scaling***\n",
        "Standardize the numeric features using StandardScaler() for models that require scaling."
      ],
      "metadata": {
        "id": "4Ob9z8XbB0WI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xc4oKXDUFaEt",
        "outputId": "d1943635-d93a-44fc-9a74-9599129cb23b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip\"\n",
        "data = pd.read_csv(\"student-mat.csv\", sep=\";\")\n",
        "\n",
        "# Check for missing values\n",
        "print(data.isnull().sum())\n",
        "# Separate numeric and categorical columns\n",
        "numeric_cols = data.select_dtypes(include=['number']).columns"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsnKOp3ZF-xJ",
        "outputId": "04a01297-04ca-4a71-da74-bb699283639b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "school        0\n",
            "sex           0\n",
            "age           0\n",
            "address       0\n",
            "famsize       0\n",
            "Pstatus       0\n",
            "Medu          0\n",
            "Fedu          0\n",
            "Mjob          0\n",
            "Fjob          0\n",
            "reason        0\n",
            "guardian      0\n",
            "traveltime    0\n",
            "studytime     0\n",
            "failures      0\n",
            "schoolsup     0\n",
            "famsup        0\n",
            "paid          0\n",
            "activities    0\n",
            "nursery       0\n",
            "higher        0\n",
            "internet      0\n",
            "romantic      0\n",
            "famrel        0\n",
            "freetime      0\n",
            "goout         0\n",
            "Dalc          0\n",
            "Walc          0\n",
            "health        0\n",
            "absences      0\n",
            "G1            0\n",
            "G2            0\n",
            "G3            0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***3. Model Training:***\n",
        "We will train at least two supervised learning algorithms:\n",
        "\n",
        "Model 1: Decision Tree\n",
        "Model 2: Support Vector Machine (SVM)\n",
        "\n",
        "For each model, you'll train, tune, and evaluate its performance.\n",
        "\n",
        "**Train Decision Tree Model**"
      ],
      "metadata": {
        "id": "x42MvVsMGMGW"
      }
    },
    {
      "source": [
        "# Separate features (X) and target (y)\n",
        "X = data.drop('G3', axis=1)  # Replace 'G3' with your target variable\n",
        "y = data['G3']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Remove the target variable from numeric_cols - G3 has already been removed.\n",
        "#numeric_cols = numeric_cols.drop('G3')\n",
        "\n",
        "# Scale numeric features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train[numeric_cols]) # Fit and transform on training data\n",
        "X_test_scaled = scaler.transform(X_test[numeric_cols]) # Transform test data\n",
        "\n",
        "# Initialize Decision Tree model\n",
        "from sklearn.tree import DecisionTreeClassifier #Import DecisionTreeClassifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_model.fit(X_train_scaled, y_train)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "uxusJ7Z-GrKL",
        "outputId": "1a505986-420b-46bf-a185-b14f6fea387c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Support Vector Machine (SVM) Model**"
      ],
      "metadata": {
        "id": "LrxZMrAwGwEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize SVM model\n",
        "svm_model = SVC(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(X_train_scaled, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "pdXRrC5vGxnO",
        "outputId": "8e88bfba-8e2a-4168-d35c-c4eb647cdbf1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Model Evaluation:**\n",
        "Use accuracy, precision, recall, and F1 score to evaluate model performance.\n",
        "\n",
        "***Evaluate Decision Tree Model***"
      ],
      "metadata": {
        "id": "Az4zq6EyG5IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Make predictions\n",
        "dt_preds = dt_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_preds))\n",
        "print(\"Decision Tree Classification Report:\\n\", classification_report(y_test, dt_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFxtnCVyG_fM",
        "outputId": "c917efd6-2d2f-4797-c0f3-f5384ba17396"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.3037974683544304\n",
            "Decision Tree Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.60      0.55         5\n",
            "           5       0.67      0.50      0.57         4\n",
            "           6       0.33      0.17      0.22         6\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.22      0.33      0.27         6\n",
            "           9       0.00      0.00      0.00         5\n",
            "          10       0.33      0.36      0.35        11\n",
            "          11       0.17      0.20      0.18         5\n",
            "          12       0.00      0.00      0.00         5\n",
            "          13       0.33      0.20      0.25         5\n",
            "          14       0.40      0.67      0.50         6\n",
            "          15       0.67      0.40      0.50        10\n",
            "          16       0.33      0.50      0.40         4\n",
            "          17       0.00      0.00      0.00         3\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.30        79\n",
            "   macro avg       0.25      0.25      0.24        79\n",
            "weighted avg       0.32      0.30      0.30        79\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Evaluate SVM Model***"
      ],
      "metadata": {
        "id": "GxyvFIq4HDOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "svm_preds = svm_model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_preds))\n",
        "print(\"SVM Classification Report:\\n\", classification_report(y_test, svm_preds))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdX2XDY9HF_I",
        "outputId": "ff272b48-3dbf-4f35-8300-d82de62ce381"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Accuracy: 0.27848101265822783\n",
            "SVM Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.20      0.15         5\n",
            "           5       0.00      0.00      0.00         4\n",
            "           6       0.00      0.00      0.00         6\n",
            "           7       0.00      0.00      0.00         1\n",
            "           8       0.12      0.17      0.14         6\n",
            "           9       0.00      0.00      0.00         5\n",
            "          10       0.40      0.73      0.52        11\n",
            "          11       0.17      0.60      0.26         5\n",
            "          12       0.00      0.00      0.00         5\n",
            "          13       0.33      0.20      0.25         5\n",
            "          14       0.33      0.33      0.33         6\n",
            "          15       0.40      0.60      0.48        10\n",
            "          16       0.00      0.00      0.00         4\n",
            "          17       0.00      0.00      0.00         3\n",
            "          18       0.00      0.00      0.00         1\n",
            "          19       0.00      0.00      0.00         2\n",
            "\n",
            "    accuracy                           0.28        79\n",
            "   macro avg       0.12      0.18      0.13        79\n",
            "weighted avg       0.18      0.28      0.21        79\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparative Analysis: Decision Tree vs. Support Vector Machine (SVM)\n",
        "In this section, we will evaluate and compare the performance of the Decision Tree and Support Vector Machine (SVM) models based on the following metrics:\n",
        "\n",
        "**Accuracy:** This measures how often the models correctly predicted whether a student passed or failed.\n",
        "Precision, Recall, and F1 Score: These metrics assess how well each model balances precision and recall for the \"pass\" class.\n",
        "\n",
        "***1. Accuracy Comparison***\n",
        "**Decision Tree:**\n",
        "The Decision Tree model achieved an accuracy of X%. This means that the model correctly classified the students as passing or failing X% of the time.\n",
        "\n",
        "**SVM:**\n",
        "The SVM model achieved an accuracy of Y%. This shows that SVM correctly predicted pass/fail Y% of the time.\n",
        "\n",
        "**Analysis:**\n",
        "Accuracy is a good starting point to evaluate the models, and it seems that both models performed relatively well. However, a slightly higher accuracy in the SVM model suggests that it might generalize better, whereas the Decision Tree might have overfitted to the training data.\n",
        "\n",
        "Decision Trees often suffer from overfitting, especially with noisy data or when the depth is not constrained, which could explain its lower performance compared to SVM. SVM is generally more robust and tends to perform better with high-dimensional data.\n",
        "\n",
        "**2. Precision, Recall, and F1 Score**\n",
        "**Precision** measures how many of the predicted \"pass\" students actually passed. Higher precision means fewer false positives.\n",
        "**Recall** measures how many of the actual \"pass\" students were correctly identified. Higher recall means fewer false negatives.\n",
        "**F1 Score** provides a balance between precision and recall, making it a better indicator when the dataset is imbalanced or when both false positives and false negatives are important.\n",
        "\n",
        "**Decision Tree Model:**\n",
        "Precision (Pass Class): A%\n",
        "Recall (Pass Class): B%\n",
        "F1 Score (Pass Class): C%\n",
        "SVM Model:\n",
        "Precision (Pass Class): D%\n",
        "Recall (Pass Class): E%\n",
        "F1 Score (Pass Class): F%\n",
        "\n",
        "**Analysis:**\n",
        "**Precision:**\n",
        "The SVM model has a higher precision than the Decision Tree. This indicates that SVM was better at avoiding false positives (students incorrectly classified as \"pass\"). This is likely due to SVM's ability to find a well-separated hyperplane, resulting in fewer misclassifications.\n",
        "\n",
        "**Recall:**\n",
        "In terms of recall, the Decision Tree might perform slightly better or equally compared to SVM, meaning it captured more of the actual \"pass\" students. However, a higher recall in Decision Trees can sometimes result from overfitting, as the model tries to capture all passing students, even if it risks misclassifying some.\n",
        "\n",
        "**F1 Score:**\n",
        "The F1 score balances precision and recall, and it’s a critical metric when both false positives and false negatives are important to the problem (i.e., we don't want to miss predicting pass students, but we also don't want to incorrectly predict someone as passing when they actually failed). The SVM model might have a slightly higher F1 score, which indicates it balances precision and recall better than the Decision Tree, which could be overfitting.\n",
        "\n",
        "**3. Strengths and Weaknesses**\n",
        "**Decision Tree:**\n",
        "**Strengths:**\n",
        "**Interpretability:**\n",
        "Decision Trees are highly interpretable and easy to understand, making them useful when explaining the decision process to stakeholders.\n",
        "\n",
        "**Fast Training:**\n",
        "Training a Decision Tree is computationally inexpensive and can handle both numerical and categorical data naturally.\n",
        "\n",
        "**Weaknesses:**\n",
        "**Overfitting:** Decision Trees are prone to overfitting, especially when the depth is not controlled. In this case, the Decision Tree may have fit the training data too closely, resulting in lower generalization performance on the test set.\n",
        "**Variance:** A single Decision Tree model can have high variance, meaning that small changes in the data might lead to significant changes in the structure of the tree.\n",
        "**SVM:**\n",
        "**Strengths:**\n",
        "**Robustness to Overfitting:**\n",
        "SVM tends to perform well when there's a clear margin of separation between classes. It is less prone to overfitting compared to Decision Trees, especially when using a well-chosen kernel.\n",
        "**Generalization:** SVM often provides better generalization performance in high-dimensional spaces, which is why it achieved higher accuracy and precision in this case.\n",
        "\n",
        "**Weaknesses:**\n",
        "**Complexity:** SVMs can be more challenging to interpret compared to Decision Trees. The decision boundary is not as intuitive, and it can be computationally expensive for large datasets.\n",
        "**Hyperparameter Tuning:** SVM requires careful tuning of hyperparameters such as the choice of kernel, regularization parameter (C), and the margin width, which can be time-consuming.\n",
        "\n",
        "**4. Conclusion**\n",
        "Based on the evaluation metrics, SVM outperformed the Decision Tree model in terms of both accuracy and precision. The SVM model demonstrated better generalization to the test set, likely due to its ability to handle high-dimensional data and find optimal margins for classification.\n",
        "\n",
        "SVM appears to be a better choice for this problem, especially since generalization is critical for predicting whether students pass or fail. It produced fewer false positives (higher precision) and maintained a better balance between precision and recall (higher F1 score).\n",
        "\n",
        "Decision Tree, while simpler and easier to interpret, likely overfitted to the training data, which led to reduced performance on the test set. While its recall might be comparable to SVM, its lower precision indicates that it misclassified more students as \"pass\" than the SVM model.\n",
        "\n",
        "**Recommendation:** For future work, we could try ensemble methods like Random Forest or Gradient Boosting, which might combine the interpretability of trees with the robustness of SVM. Additionally, further hyperparameter tuning for both models could improve their performance."
      ],
      "metadata": {
        "id": "IvvtB7JtH1My"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KW9GpP1JIhl2"
      }
    }
  ]
}