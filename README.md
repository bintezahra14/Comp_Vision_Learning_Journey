# Comp_Vision_Learning_Journey
A comprehensive portfolio that showcases my learning journey throughout the course
# Module 1: Historical Timeline of Computer Vision

## 1960s: The Dawn of Computer Vision
- **1966**: The MIT AI Lab's Summer Vision Project aimed to use computers to recognize objects and understand scenes, marking the beginning of computer vision research.

## 1970s: Early Developments and Techniques
- **1972**: David Marr developed theories on human vision and algorithms for edge detection, leading to the Marr-Hildreth algorithm.
- **1973**: The development of the first algorithm for stereo vision by Takeo Kanade.

## 1980s: Foundations and Breakthroughs
- **1980**: The publication of "Vision" by David Marr, outlining a computational theory of vision.
- **1984**: The development of the Canny edge detector by John F. Canny, providing a method for edge detection that remains influential.

## 1990s: Advancements and Practical Applications
- **1990**: Introduction of the Active Shape Model by Tim Cootes and Chris Taylor, contributing to facial recognition.
- **1995**: Development of the SIFT (Scale-Invariant Feature Transform) algorithm by David Lowe, which became a cornerstone in object recognition.

## 2000s: The Rise of Machine Learning
- **2001**: Viola-Jones face detection framework introduced by Paul Viola and Michael Jones, enabling real-time face detection.
- **2006**: Geoffrey Hinton's work on deep learning with the introduction of deep belief networks, rejuvenating neural networks.

## 2010s: Deep Learning Revolution
- **2012**: AlexNet, created by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, won the ImageNet competition, demonstrating the power of convolutional neural networks (CNNs).
- **2014**: The development of Generative Adversarial Networks (GANs) by Ian Goodfellow and colleagues, advancing image generation and manipulation.
- **2015**: Google's DeepDream project visualized what deep neural networks see, contributing to understanding neural networks.

## 2020s: Advancements in Applications and Integration
- **2020**: Significant improvements in real-time image and video analysis, self-driving cars, and facial recognition systems.
- **2021**: OpenAI's DALL-E and CLIP models showcased advanced image generation and understanding capabilities.

## Personal Reflection

As I explore the historical timeline of computer vision, I am struck by the rapid evolution and the groundbreaking advancements that have shaped this field. The journey from early edge detection algorithms in the 1970s to the sophisticated deep learning models of today underscores the transformative power of innovation.

### Key Insights:
1. **Interdisciplinary Nature**: The progression of computer vision highlights the importance of interdisciplinary research. Contributions from psychology, neuroscience, and computer science have collectively advanced our understanding and capabilities.
2. **Impact of Deep Learning**: The introduction of deep learning, particularly the success of AlexNet in 2012, marked a significant turning point. It demonstrated the potential of neural networks to handle complex visual tasks, paving the way for rapid advancements.
3. **Practical Applications**: The practical applications of computer vision, from facial recognition to autonomous driving, illustrate its impact on various industries. This reinforces my interest in applying computer vision to solve real-world problems.

### Personal Growth:
Working on this project has deepened my appreciation for the pioneers of computer vision and their contributions. It has also motivated me to stay abreast of current trends and continuously learn new techniques and methodologies. As I pursue my studies in Artificial Intelligence, I am excited about the future possibilities in this dynamic field.

### Future Aspirations:
I aim to contribute to the field of computer vision by focusing on innovative applications and improving existing models' efficiency and accuracy. By building on the foundational work of past researchers, I hope to drive advancements that can benefit society in meaningful ways.

---

This historical timeline, combined with my reflection, serves as a testament to the fascinating journey of computer vision and my evolving role within it.

# Module 02: An Overview of Computer Vision - Cameras & Sensors

## Cameras in Computer Vision

Cameras are fundamental to computer vision, as they provide the visual data needed for analysis and interpretation. The primary types of cameras used in computer vision include:

1. **RGB Cameras**: These are standard digital cameras that capture images in red, green, and blue channels. They are widely used for various applications due to their affordability and availability.
2. **Depth Cameras**: These cameras capture the distance between the camera and objects in the scene. Examples include stereo cameras, time-of-flight cameras, and structured light cameras. Depth cameras are crucial for applications like 3D modeling, gesture recognition, and autonomous driving.
3. **Infrared Cameras**: These cameras detect infrared radiation and are used in low-light conditions or to capture heat signatures. They are often used in surveillance, night vision, and medical diagnostics.
4. **Thermal Cameras**: These are a type of infrared camera that captures the thermal energy emitted by objects. They are useful for detecting heat leaks, monitoring industrial equipment, and medical imaging.

## Sensors in Computer Vision

Sensors complement cameras by providing additional data that can enhance the understanding of a scene. Common sensors used in computer vision include:

1. **LiDAR (Light Detection and Ranging)**: LiDAR sensors emit laser beams and measure the time it takes for the light to return, creating detailed 3D maps of environments. They are widely used in autonomous vehicles, robotics, and geospatial applications.
2. **IMU (Inertial Measurement Unit)**: IMUs measure the acceleration and rotation of objects, providing information about their motion and orientation. They are commonly used in mobile devices, drones, and augmented reality systems.
3. **Ultrasonic Sensors**: These sensors use sound waves to detect objects and measure distances. They are often used in robotics, obstacle avoidance systems, and industrial automation.
4. **Pressure Sensors**: These sensors detect force or pressure applied to a surface, and are used in touch-sensitive applications, robotics, and wearable devices.

## Integration of Cameras and Sensors

The integration of cameras and sensors enhances the capabilities of computer vision systems by providing richer and more diverse data. For example:
- **Autonomous Vehicles**: Use a combination of RGB cameras, LiDAR, and IMUs to navigate and understand their environment.
- **Robotics**: Utilize depth cameras, IMUs, and ultrasonic sensors to perform tasks and interact with objects.
- **Augmented Reality (AR)**: Relies on RGB cameras, depth sensors, and IMUs to overlay digital information onto the real world.

## Personal Reflection

As I delve into the world of cameras and sensors in computer vision, I am amazed by the variety of technologies and their applications. Understanding the role of different cameras and sensors has broadened my perspective on how visual data can be captured and utilized.

### Key Insights:
1. **Diverse Applications**: The diverse types of cameras and sensors and their applications highlight the versatility of computer vision. From enhancing safety in autonomous vehicles to enabling advanced medical diagnostics, the potential uses are vast and impactful.
2. **Technology Integration**: The integration of multiple sensors with cameras demonstrates the importance of combining various data sources to create more robust and accurate systems. This reinforces the concept that collaboration between different technologies can lead to superior results.
3. **Real-World Impact**: Seeing how computer vision technologies are applied in real-world scenarios inspires me to think about practical applications of my own work. It shows the tangible benefits that advanced technology can bring to everyday life.

### Personal Growth:
Exploring this module has deepened my understanding of the technical aspects of computer vision and the importance of selecting appropriate cameras and sensors for specific applications. It has also sparked my curiosity to learn more about how these technologies are developed and optimized.

### Future Aspirations:
I aspire to work on projects that integrate various types of cameras and sensors to create innovative solutions. By understanding the strengths and limitations of each technology, I aim to contribute to the development of more effective and efficient computer vision systems.

---

This overview of cameras and sensors, combined with my reflection, illustrates the foundational role these technologies play in computer vision and my growing expertise in this area.

# Module 03: Tools of the Trade

## Lab Experience Reflection

### What I Did
During the lab session, I undertook several key activities to set up and work within a new computing environment, facing many challenges as I had no prior exposure to installing multiple programs and running them together:

1. **Setting Up GitHub Account:**
   - I created a new GitHub account by visiting the GitHub website, registering with my email, and setting up a username and password.

2. **Creating a Repository:**
   - After setting up my GitHub account, I created a new repository. This involved choosing a repository name, deciding on its visibility (public or private), and initializing it with a README file.

3. **Installing Jupyter Notebook:**
   - I installed Jupyter Notebook using Python 3, which simplifies package management and deployment. This involved downloading Python 3, installing it, and verifying the installation by running Jupyter Notebook from the command line.

4. **Setting Up Visual Studio Code:**
   - I installed Visual Studio Code (VS Code) as my code editor. This included downloading VS Code, installing it, and adding necessary extensions for Python and Jupyter Notebooks.

5. **Performing Basic Operations in Jupyter Notebook with VS Code and Python 3:**
   - Within VS Code, I created and opened a new Jupyter Notebook.
   - I wrote and executed basic Python code cells to familiarize myself with the interactive computing environment.
   - I performed basic operations such as importing libraries, creating variables, performing calculations, and visualizing data using simple plots.

### What I Learned
The lab introduced me to several new concepts and tools, each of which plays a crucial role in modern software development and data science:

1. **Version Control with GitHub:**
   - I learned about the importance of version control and how GitHub helps manage and track changes in code. It provides a platform for collaboration and ensures that all contributions are documented and reversible if necessary.

2. **Interactive Computing with Jupyter Notebooks:**
   - Jupyter Notebooks provide an interactive environment where code can be written, executed, and documented in a single interface. This is particularly useful for data analysis, visualization, and sharing results in an easily readable format.

3. **Challenges and Solutions:**
   - One significant challenge I faced was linking Jupyter Notebook to GitHub. The process was complex, involving numerous installations and extensions. Despite following multiple tutorials on YouTube, I found the setup to be quite tough.
   - To overcome this, I meticulously followed step-by-step guides, ensured all necessary software and extensions were correctly installed, and tested each step to verify successful integration.

### Conclusion
This lab experience was both challenging and rewarding. Setting up and using these tools has provided me with a foundational understanding of essential practices in version control and interactive computing. Although linking Jupyter Notebook to GitHub was difficult, the persistence paid off, and I now feel more confident in managing and sharing my work through these platforms. This experience has also highlighted the importance of utilizing community resources and step-by-step guides to navigate complex technical setups.

**Please find the link to my GitHub repository:**
[bintezahra14/jupyter-exploration: First assignment to link Jupyter notebook to GitHub](https://github.com/bintezahra14/jupyter-exploration)




