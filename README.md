# Comp_Vision_Learning_Journey
A comprehensive portfolio that showcases my learning journey throughout the course
# Module 1: Historical Timeline of Computer Vision

## 1960s: The Dawn of Computer Vision
- **1966**: The MIT AI Lab's Summer Vision Project aimed to use computers to recognize objects and understand scenes, marking the beginning of computer vision research.

## 1970s: Early Developments and Techniques
- **1972**: David Marr developed theories on human vision and algorithms for edge detection, leading to the Marr-Hildreth algorithm.
- **1973**: The development of the first algorithm for stereo vision by Takeo Kanade.

## 1980s: Foundations and Breakthroughs
- **1980**: The publication of "Vision" by David Marr, outlining a computational theory of vision.
- **1984**: The development of the Canny edge detector by John F. Canny, providing a method for edge detection that remains influential.

## 1990s: Advancements and Practical Applications
- **1990**: Introduction of the Active Shape Model by Tim Cootes and Chris Taylor, contributing to facial recognition.
- **1995**: Development of the SIFT (Scale-Invariant Feature Transform) algorithm by David Lowe, which became a cornerstone in object recognition.

## 2000s: The Rise of Machine Learning
- **2001**: Viola-Jones face detection framework introduced by Paul Viola and Michael Jones, enabling real-time face detection.
- **2006**: Geoffrey Hinton's work on deep learning with the introduction of deep belief networks, rejuvenating neural networks.

## 2010s: Deep Learning Revolution
- **2012**: AlexNet, created by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, won the ImageNet competition, demonstrating the power of convolutional neural networks (CNNs).
- **2014**: The development of Generative Adversarial Networks (GANs) by Ian Goodfellow and colleagues, advancing image generation and manipulation.
- **2015**: Google's DeepDream project visualized what deep neural networks see, contributing to understanding neural networks.

## 2020s: Advancements in Applications and Integration
- **2020**: Significant improvements in real-time image and video analysis, self-driving cars, and facial recognition systems.
- **2021**: OpenAI's DALL-E and CLIP models showcased advanced image generation and understanding capabilities.

## Personal Reflection

As I explore the historical timeline of computer vision, I am struck by the rapid evolution and the groundbreaking advancements that have shaped this field. The journey from early edge detection algorithms in the 1970s to the sophisticated deep learning models of today underscores the transformative power of innovation.

### Key Insights:
1. **Interdisciplinary Nature**: The progression of computer vision highlights the importance of interdisciplinary research. Contributions from psychology, neuroscience, and computer science have collectively advanced our understanding and capabilities.
2. **Impact of Deep Learning**: The introduction of deep learning, particularly the success of AlexNet in 2012, marked a significant turning point. It demonstrated the potential of neural networks to handle complex visual tasks, paving the way for rapid advancements.
3. **Practical Applications**: The practical applications of computer vision, from facial recognition to autonomous driving, illustrate its impact on various industries. This reinforces my interest in applying computer vision to solve real-world problems.

### Personal Growth:
Working on this project has deepened my appreciation for the pioneers of computer vision and their contributions. It has also motivated me to stay abreast of current trends and continuously learn new techniques and methodologies. As I pursue my studies in Artificial Intelligence, I am excited about the future possibilities in this dynamic field.

### Future Aspirations:
I aim to contribute to the field of computer vision by focusing on innovative applications and improving existing models' efficiency and accuracy. By building on the foundational work of past researchers, I hope to drive advancements that can benefit society in meaningful ways.

---

This historical timeline, combined with my reflection, serves as a testament to the fascinating journey of computer vision and my evolving role within it.

# Module 02: An Overview of Computer Vision - Cameras & Sensors

## Cameras in Computer Vision

Cameras are fundamental to computer vision, as they provide the visual data needed for analysis and interpretation. The primary types of cameras used in computer vision include:

1. **RGB Cameras**: These are standard digital cameras that capture images in red, green, and blue channels. They are widely used for various applications due to their affordability and availability.
2. **Depth Cameras**: These cameras capture the distance between the camera and objects in the scene. Examples include stereo cameras, time-of-flight cameras, and structured light cameras. Depth cameras are crucial for applications like 3D modeling, gesture recognition, and autonomous driving.
3. **Infrared Cameras**: These cameras detect infrared radiation and are used in low-light conditions or to capture heat signatures. They are often used in surveillance, night vision, and medical diagnostics.
4. **Thermal Cameras**: These are a type of infrared camera that captures the thermal energy emitted by objects. They are useful for detecting heat leaks, monitoring industrial equipment, and medical imaging.

## Sensors in Computer Vision

Sensors complement cameras by providing additional data that can enhance the understanding of a scene. Common sensors used in computer vision include:

1. **LiDAR (Light Detection and Ranging)**: LiDAR sensors emit laser beams and measure the time it takes for the light to return, creating detailed 3D maps of environments. They are widely used in autonomous vehicles, robotics, and geospatial applications.
2. **IMU (Inertial Measurement Unit)**: IMUs measure the acceleration and rotation of objects, providing information about their motion and orientation. They are commonly used in mobile devices, drones, and augmented reality systems.
3. **Ultrasonic Sensors**: These sensors use sound waves to detect objects and measure distances. They are often used in robotics, obstacle avoidance systems, and industrial automation.
4. **Pressure Sensors**: These sensors detect force or pressure applied to a surface, and are used in touch-sensitive applications, robotics, and wearable devices.

## Integration of Cameras and Sensors

The integration of cameras and sensors enhances the capabilities of computer vision systems by providing richer and more diverse data. For example:
- **Autonomous Vehicles**: Use a combination of RGB cameras, LiDAR, and IMUs to navigate and understand their environment.
- **Robotics**: Utilize depth cameras, IMUs, and ultrasonic sensors to perform tasks and interact with objects.
- **Augmented Reality (AR)**: Relies on RGB cameras, depth sensors, and IMUs to overlay digital information onto the real world.

## Personal Reflection

As I delve into the world of cameras and sensors in computer vision, I am amazed by the variety of technologies and their applications. Understanding the role of different cameras and sensors has broadened my perspective on how visual data can be captured and utilized.

### Key Insights:
1. **Diverse Applications**: The diverse types of cameras and sensors and their applications highlight the versatility of computer vision. From enhancing safety in autonomous vehicles to enabling advanced medical diagnostics, the potential uses are vast and impactful.
2. **Technology Integration**: The integration of multiple sensors with cameras demonstrates the importance of combining various data sources to create more robust and accurate systems. This reinforces the concept that collaboration between different technologies can lead to superior results.
3. **Real-World Impact**: Seeing how computer vision technologies are applied in real-world scenarios inspires me to think about practical applications of my own work. It shows the tangible benefits that advanced technology can bring to everyday life.

### Personal Growth:
Exploring this module has deepened my understanding of the technical aspects of computer vision and the importance of selecting appropriate cameras and sensors for specific applications. It has also sparked my curiosity to learn more about how these technologies are developed and optimized.

### Future Aspirations:
I aspire to work on projects that integrate various types of cameras and sensors to create innovative solutions. By understanding the strengths and limitations of each technology, I aim to contribute to the development of more effective and efficient computer vision systems.

---

This overview of cameras and sensors, combined with my reflection, illustrates the foundational role these technologies play in computer vision and my growing expertise in this area.

# Module 03: Tools of the Trade

## Lab Experience Reflection

### What I Did
During the lab session, I undertook several key activities to set up and work within a new computing environment, facing many challenges as I had no prior exposure to installing multiple programs and running them together:

1. **Setting Up GitHub Account:**
   - I created a new GitHub account by visiting the GitHub website, registering with my email, and setting up a username and password.

2. **Creating a Repository:**
   - After setting up my GitHub account, I created a new repository. This involved choosing a repository name, deciding on its visibility (public or private), and initializing it with a README file.

3. **Installing Jupyter Notebook:**
   - I installed Jupyter Notebook using Python 3, which simplifies package management and deployment. This involved downloading Python 3, installing it, and verifying the installation by running Jupyter Notebook from the command line.

4. **Setting Up Visual Studio Code:**
   - I installed Visual Studio Code (VS Code) as my code editor. This included downloading VS Code, installing it, and adding necessary extensions for Python and Jupyter Notebooks.

5. **Performing Basic Operations in Jupyter Notebook with VS Code and Python 3:**
   - Within VS Code, I created and opened a new Jupyter Notebook.
   - I wrote and executed basic Python code cells to familiarize myself with the interactive computing environment.
   - I performed basic operations such as importing libraries, creating variables, performing calculations, and visualizing data using simple plots.

### What I Learned
The lab introduced me to several new concepts and tools, each of which plays a crucial role in modern software development and data science:

1. **Version Control with GitHub:**
   - I learned about the importance of version control and how GitHub helps manage and track changes in code. It provides a platform for collaboration and ensures that all contributions are documented and reversible if necessary.

2. **Interactive Computing with Jupyter Notebooks:**
   - Jupyter Notebooks provide an interactive environment where code can be written, executed, and documented in a single interface. This is particularly useful for data analysis, visualization, and sharing results in an easily readable format.

3. **Challenges and Solutions:**
   - One significant challenge I faced was linking Jupyter Notebook to GitHub. The process was complex, involving numerous installations and extensions. Despite following multiple tutorials on YouTube, I found the setup to be quite tough.
   - To overcome this, I meticulously followed step-by-step guides, ensured all necessary software and extensions were correctly installed, and tested each step to verify successful integration.

### Conclusion
This lab experience was both challenging and rewarding. Setting up and using these tools has provided me with a foundational understanding of essential practices in version control and interactive computing. Although linking Jupyter Notebook to GitHub was difficult, the persistence paid off, and I now feel more confident in managing and sharing my work through these platforms. This experience has also highlighted the importance of utilizing community resources and step-by-step guides to navigate complex technical setups.

**Please find the link to my GitHub repository:**
[bintezahra14/jupyter-exploration: First assignment to link Jupyter notebook to GitHub](https://github.com/bintezahra14/jupyter-exploration)


# Module 04: ITAI 1378 Fundamentals of Image Processing

## Lab Experience Reflection

### A04 "Image Processing Adventure Quest: A Butterfly's New Life"

#### Characters:
- **Jaya**: A young photographer
- **Quyen**: Jaya's friend and fellow photographer
- **Mr. Lens**: A wise, mystical photo studio owner

#### Scene 1: Jaya's Photo Studio
*(Jaya is sitting at her desk, looking frustrated with her laptop. Quyen walks in.)*

**Quyen**: Hey, Jaya! You look like you’ve been staring at that screen for hours. What’s wrong?  
**Jaya**: (sighs) I took this photo of a butterfly, but it looks so dull and blurry. I’ve tried everything I know, but it still doesn’t look right.  
**Quyen**: Maybe you need some advanced techniques. How about we visit Mr. Lens’ Enchanted Photo Studio again? He might have the solution.  
**Jaya**: (perks up) That’s a great idea! Let’s go!

#### Scene 2: Mr. Lens’ Enchanted Photo Studio
*(Jaya and Quyen enter a quaint, mystical studio filled with photographs glowing with vibrant colors. Mr. Lens, an elderly man with a kind smile, stands behind the counter.)*

**Mr. Lens**: Welcome back, Jaya and Quyen! How can I assist you today?  
**Jaya**: Mr. Lens, I took this photo of a butterfly, but it looks so dull and blurry. Can you help me bring out its true beauty?

---

Diving into the world of image processing was like unlocking a treasure chest of cool techniques and tools. It really opened my eyes to how you can transform a simple photo into something stunning. I enjoy taking pictures, and this was a great experience to learn editing tools that I can use to enhance my pictures. The picture we used over here was taken by me. I worked with three main techniques: histogram equalization, smoothing, and sharpening, and each one taught me something new.

### Histogram Equalization
The first technique I used is called histogram equalization. Basically, it’s all about adjusting the contrast by redistributing the light and dark areas more evenly across the image. When I applied histogram equalization to my butterfly photo, the colors popped, and all those hidden details came to life. It made me realize how important contrast is in making an image look balanced and vibrant.

### Smoothing
Next, I tackled smoothing, which is used for reducing noise. We know that the picture comes with some amount to a lot of grains that can mess up a photo, especially in low light. Smoothing helps get rid of those by softening the picture. When I applied it to my image, everything looked much cleaner. But I also learned that you have to be careful not to overdo it. It’s about finding the right balance.

### Sharpening
Finally, I worked with sharpening, which is about making the details stand out by emphasizing the edges. When I used sharpening on the butterfly picture, the wings and body became so much more detailed. Too much sharpening can create weird halos around the edges and make the image look unnatural. The key lesson here is subtlety.

### Key Lessons
Throughout this whole process, one theme kept coming up: balance. Whether it was adjusting contrast, reducing noise, or enhancing details, finding the right balance was crucial. And one of the most important things I learned was the value of non-destructive editing. It’s a best practice in image processing because it lets you experiment without losing the original quality.

### Broader Applications
This journey wasn’t just about making pretty pictures. These techniques are used everywhere – from photography and semiconductor industries to medical imaging. These tasks require complex image transformations, filtering, and enhancements. For example, in the medical field, these imaging techniques are crucial for things like MRI scans and X-rays, helping to reveal important details that can save lives.

### Conclusion
Overall, this quest into the fundamentals of image processing was incredibly rewarding. It gave me a deeper understanding of how techniques like histogram equalization, smoothing, and sharpening can dramatically enhance image quality. Each technique taught me valuable lessons about balance, subtlety, and the importance of non-destructive editing. Whether it is for photography, industrial applications, or medical imaging, these foundational techniques are essential tools for anyone working with images.

# Module 05: Machine Learning for Computer Vision

## Image Classification with SVM - Support Vector Machine Learning Lab

### Lab Experience Reflection

In this lab session, I explored the use of Support Vector Machines (SVM) for image classification. SVM is a powerful supervised learning algorithm often used for classification tasks. Here’s a breakdown of what I did and learned during the lab:

### What I Did

1. **Dataset Preparation:**
   - I used the CIFAR-10 dataset, which consists of 60,000 32x32 color images in 10 different classes, with 6,000 images per class. The dataset is divided into 50,000 training images and 10,000 testing images.

2. **Data Preprocessing:**
   - Loaded the CIFAR-10 dataset and performed data normalization to scale the pixel values to the range [0, 1].
   - Flattened the images to convert the 32x32x3 pixel arrays into a single 1-dimensional array with 3,072 elements.

3. **Feature Extraction:**
   - Extracted features from the images using techniques like Histogram of Oriented Gradients (HOG) to capture essential information while reducing the dimensionality of the data.

4. **Model Training:**
   - Implemented an SVM classifier using the `scikit-learn` library.
   - Trained the SVM model on the training data with different kernel functions (linear, polynomial, RBF) to see which one performed best.

5. **Model Evaluation:**
   - Evaluated the trained SVM model on the test data.
   - Computed metrics such as accuracy, precision, recall, and F1-score to assess the performance of the classifier.

### What I Learned

1. **Understanding SVM:**
   - Learned the fundamental principles of SVM, including the concepts of hyperplanes, support vectors, and margins.
   - Understood how SVM aims to find the optimal hyperplane that maximizes the margin between different classes in the feature space.

2. **Kernel Functions:**
   - Gained insight into different kernel functions (linear, polynomial, RBF) and their roles in transforming the input space to make the data linearly separable.
   - Discovered that the RBF kernel often performs well for complex datasets due to its ability to handle non-linear relationships.

3. **Challenges and Solutions:**
   - Faced challenges with computational efficiency due to the high dimensionality of the image data.
   - Learned to optimize the SVM model by tuning hyperparameters such as the regularization parameter (C) and the kernel parameters (gamma).

### Reflection

This lab session provided a hands-on experience with SVM for image classification, highlighting both its strengths and limitations. The CIFAR-10 dataset offered a rich and diverse set of images, making it an excellent choice for experimenting with different machine learning techniques. 

Through this exercise, I learned the importance of feature extraction in reducing the dimensionality of image data and improving model performance. The choice of kernel functions significantly impacts the classifier's ability to generalize from the training data to unseen test data.

One of the key takeaways from this lab was the iterative process of model tuning and evaluation. By experimenting with different hyperparameters and kernel functions, I was able to achieve a better understanding of how SVM works and how to adapt it to various datasets.

Overall, this lab experience enhanced my knowledge of machine learning techniques for computer vision and provided practical skills that will be invaluable in future projects. The journey of training and optimizing an SVM model for image classification was challenging but highly rewarding.

---

## GitHub Repository

For more details and the complete code, visit my GitHub repository: [bintezahra14/image-classification-svm](https://github.com/bintezahra14/image-classification-svm)

# Module 06: ITAI 1378 Basics of Neural Networks

## A06 TensorFlow Playground Presentation

### Overview

In this module, I delved into the basics of neural networks using TensorFlow Playground, an interactive visualization tool that helps in understanding how neural networks operate. This hands-on experience was aimed at grasping the fundamental concepts of neural networks, including layers, activation functions, and how these components work together to perform complex tasks.

### Insights and Reflection

Using TensorFlow Playground provided a visual and intuitive way to see how neural networks learn and make predictions. Here are some key insights and reflections from the experience:

1. **Understanding Neural Networks:**
   - Neural networks consist of layers of nodes (neurons), each layer transforming the input data through weighted connections.
   - The first layer (input layer) takes the input features, while the final layer (output layer) provides the predictions.
   - Hidden layers between the input and output layers help in learning complex patterns and representations.

2. **Activation Functions:**
   - Activation functions introduce non-linearity into the network, enabling it to learn and represent more complex functions.
   - Common activation functions include ReLU (Rectified Linear Unit), Sigmoid, and Tanh.
   - Observing the effect of different activation functions on the learning process was enlightening, as it showed how they influence the convergence and accuracy of the model.

3. **Training and Loss Functions:**
   - Training a neural network involves adjusting the weights using backpropagation to minimize the loss function.
   - The loss function measures the difference between the predicted and actual values. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy for classification tasks.
   - Visualizing the loss landscape and how the network learns to minimize it was a crucial part of understanding the training process.

4. **Overfitting and Regularization:**
   - Overfitting occurs when the model learns the training data too well, including the noise, which negatively impacts its performance on unseen data.
   - Techniques like dropout (randomly setting some neurons to zero during training) and L2 regularization (penalizing large weights) help in preventing overfitting.
   - Experimenting with these techniques on TensorFlow Playground highlighted their importance in building robust models.

5. **Learning Rate and Optimization:**
   - The learning rate controls how much the model's weights are updated with each iteration.
   - Choosing an appropriate learning rate is crucial; too high can cause the model to converge too quickly to a suboptimal solution, while too low can make the training process very slow.
   - Observing the impact of different learning rates on the training process provided valuable insights into optimization.

## Lab 06: Chihuahua or Muffin Workshop

### Lab Experience Reflection

This lab involved a fun and practical exercise known as the "Chihuahua or Muffin" challenge, where I used neural networks to classify images as either a Chihuahua dog or a muffin. The goal was to understand the challenges of image classification and the importance of neural networks in tackling these challenges.

### What I Did

1. **Data Collection:**
   - Collected images of Chihuahuas and muffins from an online dataset. This dataset provided a good mix of both categories, which are notoriously difficult to distinguish due to their visual similarities.

2. **Data Preprocessing:**
   - Preprocessed the images by resizing them to a standard size, normalizing the pixel values, and converting them into a format suitable for training a neural network.

3. **Building the Neural Network:**
   - Used TensorFlow and Keras to build a Convolutional Neural Network (CNN) tailored for image classification.
   - The CNN architecture included several convolutional layers, activation functions, pooling layers, and fully connected layers.

4. **Training the Model:**
   - Trained the CNN model on the preprocessed dataset, using techniques like data augmentation to enhance the diversity of the training data.
   - Monitored the training process by tracking metrics like accuracy and loss on both the training and validation datasets.

5. **Evaluating the Model:**
   - Evaluated the trained model on a separate test dataset to assess its performance.
   - Used confusion matrices and classification reports to understand the model’s strengths and weaknesses.

### What I Learned

1. **Image Classification Challenges:**
   - The "Chihuahua or Muffin" challenge highlighted the difficulty of distinguishing between visually similar objects, emphasizing the importance of advanced image processing techniques.

2. **CNN Architecture:**
   - Learned about the different components of CNNs, including convolutional layers that extract features, pooling layers that reduce dimensionality, and fully connected layers that perform the final classification.
   - Understood how the depth and complexity of the network affect its ability to learn and generalize from the data.

3. **Data Augmentation:**
   - Realized the importance of data augmentation in enhancing the robustness of the model by artificially increasing the diversity of the training data.
   - Techniques like rotation, flipping, and zooming helped in making the model more resilient to variations in the input data.

4. **Model Evaluation:**
   - Learned to evaluate the model's performance using metrics like accuracy, precision, recall, and F1-score.
   - Identified the significance of using confusion matrices to get a detailed view of the model’s performance on different classes.

### Reflection

This workshop was an excellent opportunity to apply the theoretical knowledge of neural networks to a practical and engaging problem. The experience reinforced my understanding of CNNs and their application in image classification tasks. The challenge of distinguishing between Chihuahuas and muffins underscored the power and complexity of neural networks in handling real-world classification problems.

Overall, this module and workshop have significantly enhanced my understanding of neural networks, from the basics to their practical applications in computer vision. I feel more confident in using TensorFlow and Keras to build and train neural networks for various tasks, and I look forward to exploring more advanced topics in this field.

---

## GitHub Repository

For more details and the complete code, visit my GitHub repository: [bintezahra14/chihuahua-or-muffin](https://github.com/bintezahra14/chihuahua-or-muffin)



